{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def load_merge_data(messages_filepath, categories_filepath):\n",
    "\n",
    "    messages_filepath = 'data/disaster_messages.csv'\n",
    "    categories_filepath = 'data/disaster_categories.csv'\n",
    "\n",
    "    #Reads disaster_messages.csv and drop the original column\n",
    "    df_mess = pd.read_csv(messages_filepath, encoding='latin-1')\n",
    "    df_mess.drop(['original'],axis=1,inplace=True)\n",
    "    \n",
    "    #Reads disaster_categories.csv\n",
    "    df_cat = pd.read_csv(categories_filepath, encoding='latin-1')\n",
    "\n",
    "    # Merges both dataframes on ['Id']\n",
    "    df = df_mess.merge(df_cat, how='outer', on=['id'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "### Creates columns with correspondent values of the 'categories' column\n",
    "\n",
    "    # Provides a list with all the columns extracted from the category column\n",
    "    df.dropna()\n",
    "    cat = df.loc[0,'categories']\n",
    "    cat_list = cat.split(';')\n",
    "    col_names = []\n",
    "    for val in cat_list:\n",
    "        c = val.split('-')[0]\n",
    "        col_names.append(c)\n",
    "\n",
    "    # Creates all columns in df with correct value\n",
    "    for col in col_names[0:-1]:\n",
    "        try:\n",
    "            df[col]          = df['categories'].apply(lambda st: st[st.find(\"-\")+1:st.find(\";\")])\n",
    "            df['categories'] = df['categories'].str.split(';',n=1).str[1:]\n",
    "            df['categories'] = df['categories'].apply(lambda x: str(x[0]))\n",
    "        # deals with the last column\n",
    "        except:\n",
    "            df[col_names[-1]]= df['categories'].apply(lambda st: st[st.find(\"-\")+1:])\n",
    "    \n",
    "    # Drops de 'categories' column\n",
    "    df.drop(['categories'], axis = 1, inplace = True)\n",
    "    \n",
    "    #Remove duplicates\n",
    "    print('Number of columns: {}, and number of duplicates: {}'.format(df['message'].shape[0],df[df.duplicated() == True]['id'].count()))\n",
    "    print('The following rows are duplications:')\n",
    "    print(df[['id','message']][df.duplicated()==True])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    #df.drop_duplicates(subset=['id', 'message'],inplace=True)\n",
    "    print('Number of columns of new dataframe excluding duplicates: {}'.format(df['message'].shape[0]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 26386, and number of duplicates: 172\n",
      "The following rows are duplications:\n",
      "          id                                            message\n",
      "164      202  ?? port au prince ?? and food. they need gover...\n",
      "165      202  ?? port au prince ?? and food. they need gover...\n",
      "658      804  elle est vraiment malade et a besoin d'aide. u...\n",
      "659      804  elle est vraiment malade et a besoin d'aide. u...\n",
      "660      804  elle est vraiment malade et a besoin d'aide. u...\n",
      "...      ...                                                ...\n",
      "25291  29022  In a field in Jallouzai, just inside Pakistan,...\n",
      "25292  29022  In a field in Jallouzai, just inside Pakistan,...\n",
      "25378  29119  Most victims (90 per cent) show little or no s...\n",
      "25379  29119  Most victims (90 per cent) show little or no s...\n",
      "25380  29119  Most victims (90 per cent) show little or no s...\n",
      "\n",
      "[172 rows x 2 columns]\n",
      "Number of columns of new dataframe excluding duplicates: 26214\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Load and merge datasets\n",
    "    messages_filepath = 'data/disaster_messages.csv'\n",
    "    categories_filepath = 'data/disaster_categories.csv'\n",
    "\n",
    "    df = load_merge_data(messages_filepath, categories_filepath)\n",
    "\n",
    "    # Clean and remove duplicates\n",
    "    df = clean_data(df)\n",
    "\n",
    "    # Save clean dataset into an sqlite database\n",
    "    engine = create_engine('sqlite:///Disaster_response_pipelines.db')\n",
    "    df.to_sql('Disaster_response_pipelines', engine, index=False, if_exists = 'replace')\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rodrigo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rodrigo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    engine = create_engine('sqlite:///Disaster_response_pipelines.db')\n",
    "    df = pd.read_sql_table('Disaster_response_pipelines', con=engine)#, 'sqlite:///Disaster_response_pipelines.db')\n",
    "    df = df[df['related'] != '2']\n",
    "    X = df['message']\n",
    "    y= df[df.columns[4:]].apply(pd.to_numeric, errors ='ignore')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('data/DisasterResponse.db', con=engine)\n",
    "df = df[df['related'] != '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'message', 'genre', 'related', 'request', 'offer', 'aid_related',\n",
       "       'medical_help', 'medical_products', 'search_and_rescue', 'security',\n",
       "       'military', 'child_alone', 'water', 'food', 'shelter', 'clothing',\n",
       "       'money', 'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    # Normalize text and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    words = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    tokens = [w for w in tokens if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lemmed = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    #clean_tokens = []\n",
    "    #for tok in tokens:\n",
    "    #    clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "    #    clean_tokens.append(clean_tok)\n",
    "\n",
    "    return lemmed#clean_tokens\n",
    "\n",
    "#X, y, cat_names = load_data()\n",
    "#for message in X[:5]:\n",
    "#    tokens = tokenize(message)\n",
    "#    print(message)\n",
    "#    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf',  MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    # to check parameters\n",
    "    # pipeline.get_params()\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "        'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        #'vect__max_df': (0.5, 1.0),\n",
    "        #'vect__max_features': (None, 5000, 10000),\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        #'vect__max_features': (None, 5000),\n",
    "        'clf__estimator__n_estimators': [10]\n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters, cv=2, verbose=3)\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    pickle.dump(model, open(model_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.392, total= 2.9min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.385, total= 2.9min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.395, total= 3.3min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.403, total= 3.2min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.397, total= 3.0min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.398, total= 2.9min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.403, total= 3.2min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.399, total= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 24.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               request       0.80      0.44      0.57      1120\n",
      "                 offer       0.00      0.00      0.00        27\n",
      "           aid_related       0.76      0.55      0.64      2682\n",
      "          medical_help       0.58      0.05      0.09       510\n",
      "      medical_products       0.88      0.16      0.27       328\n",
      "     search_and_rescue       0.69      0.05      0.10       169\n",
      "              security       0.33      0.01      0.01       131\n",
      "              military       0.60      0.03      0.06       205\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.87      0.27      0.41       401\n",
      "                  food       0.81      0.54      0.65       746\n",
      "               shelter       0.81      0.23      0.36       570\n",
      "              clothing       0.82      0.08      0.15       106\n",
      "                 money       1.00      0.03      0.06       149\n",
      "        missing_people       1.00      0.03      0.06        60\n",
      "              refugees       0.30      0.01      0.03       226\n",
      "                 death       0.82      0.12      0.21       295\n",
      "             other_aid       0.47      0.03      0.06       868\n",
      "infrastructure_related       0.20      0.00      0.01       403\n",
      "             transport       0.62      0.03      0.05       278\n",
      "             buildings       0.85      0.12      0.21       329\n",
      "           electricity       0.89      0.06      0.11       139\n",
      "                 tools       0.00      0.00      0.00        41\n",
      "             hospitals       0.00      0.00      0.00        67\n",
      "                 shops       0.00      0.00      0.00        36\n",
      "           aid_centers       0.50      0.02      0.03        56\n",
      "  other_infrastructure       0.00      0.00      0.00       275\n",
      "       weather_related       0.84      0.53      0.65      1795\n",
      "                floods       0.91      0.26      0.40       497\n",
      "                 storm       0.72      0.34      0.46       589\n",
      "                  fire       0.33      0.02      0.03        59\n",
      "            earthquake       0.91      0.54      0.68       624\n",
      "                  cold       0.83      0.07      0.13       143\n",
      "         other_weather       0.50      0.04      0.08       343\n",
      "\n",
      "             micro avg       0.79      0.32      0.45     14267\n",
      "             macro avg       0.58      0.14      0.19     14267\n",
      "          weighted avg       0.71      0.32      0.40     14267\n",
      "           samples avg       0.36      0.19      0.23     14267\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rodrigo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Rodrigo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Rodrigo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Rodrigo\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # build model\n",
    "    model = build_model()\n",
    "\n",
    "    # train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Save model\n",
    "    save_model(model, model_filepath = 'test.sav')\n",
    "\n",
    "    # display results\n",
    "    print(classification_report(y_test, y_pred, target_names=y.columns))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
