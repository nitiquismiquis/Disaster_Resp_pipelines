{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def load_merge_data(messages_filepath, categories_filepath):\n",
    "\n",
    "    messages_filepath = 'data/disaster_messages.csv'\n",
    "    categories_filepath = 'data/disaster_categories.csv'\n",
    "\n",
    "    #Reads disaster_messages.csv and drop the original column\n",
    "    df_mess = pd.read_csv(messages_filepath, encoding='latin-1')\n",
    "    df_mess.drop(['original'],axis=1,inplace=True)\n",
    "    \n",
    "    #Reads disaster_categories.csv\n",
    "    df_cat = pd.read_csv(categories_filepath, encoding='latin-1')\n",
    "\n",
    "    # Merges both dataframes on ['Id']\n",
    "    df = df_mess.merge(df_cat, how='outer', on=['id'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "### Creates columns with correspondent values of the 'categories' column\n",
    "\n",
    "    # Provides a list with all the columns extracted from the category column\n",
    "    cat = df.loc[0,'categories']\n",
    "    cat_list = cat.split(';')\n",
    "    col_names = []\n",
    "    for val in cat_list:\n",
    "        c = val.split('-')[0]\n",
    "        col_names.append(c)\n",
    "\n",
    "    # Creates all columns in df with correct value\n",
    "    for col in col_names[0:-1]:\n",
    "        try:\n",
    "            df[col]          = df['categories'].apply(lambda st: st[st.find(\"-\")+1:st.find(\";\")])\n",
    "            df['categories'] = df['categories'].str.split(';',n=1).str[1:]\n",
    "            df['categories'] = df['categories'].apply(lambda x: str(x[0]))\n",
    "        # deals with the last column\n",
    "        except:\n",
    "            df[col_names[-1]]= df['categories'].apply(lambda st: st[st.find(\"-\")+1:])\n",
    "    \n",
    "    # Drops de 'categories' column\n",
    "    df.drop(['categories'], axis = 1, inplace = True)\n",
    "    \n",
    "    #Remove duplicates\n",
    "    print('Number of columns: {}, and number of duplicates: {}'.format(df['message'].shape[0],df[df.duplicated() == True]['id'].count()))\n",
    "    print('The following rows are duplications:')\n",
    "    print(df[['id','message']][df.duplicated()==True])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    #df.drop_duplicates(subset=['id', 'message'],inplace=True)\n",
    "    print('Number of columns of new dataframe excluding duplicates: {}'.format(df['message'].shape[0]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 26386, and number of duplicates: 172\n",
      "The following rows are duplications:\n",
      "          id                                            message\n",
      "164      202  ?? port au prince ?? and food. they need gover...\n",
      "165      202  ?? port au prince ?? and food. they need gover...\n",
      "658      804  elle est vraiment malade et a besoin d'aide. u...\n",
      "659      804  elle est vraiment malade et a besoin d'aide. u...\n",
      "660      804  elle est vraiment malade et a besoin d'aide. u...\n",
      "...      ...                                                ...\n",
      "25291  29022  In a field in Jallouzai, just inside Pakistan,...\n",
      "25292  29022  In a field in Jallouzai, just inside Pakistan,...\n",
      "25378  29119  Most victims (90 per cent) show little or no s...\n",
      "25379  29119  Most victims (90 per cent) show little or no s...\n",
      "25380  29119  Most victims (90 per cent) show little or no s...\n",
      "\n",
      "[172 rows x 2 columns]\n",
      "Number of columns of new dataframe excluding duplicates: 26214\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Load and merge datasets\n",
    "    messages_filepath = 'data/disaster_messages.csv'\n",
    "    categories_filepath = 'data/disaster_categories.csv'\n",
    "\n",
    "    df = load_merge_data(messages_filepath, categories_filepath)\n",
    "\n",
    "    # Clean and remove duplicates\n",
    "    df = clean_data(df)\n",
    "\n",
    "    # Save clean dataset into an sqlite database\n",
    "    engine = create_engine('sqlite:///Disaster_response_pipelines.db')\n",
    "    df.to_sql('Disaster_response_pipelines', engine, index=False)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
